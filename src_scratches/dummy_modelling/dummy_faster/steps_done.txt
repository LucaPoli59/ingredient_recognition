test done on mexican dataset with input size 224x224 and batch size 128 for 2 epochs
with 10 epochs: 571s
with 2 epochs: 105s

Steps:

- Persistent_workers, Pin_memory, num_workers=2 [--]
- Chain operations over the forward pass [--]
- Using trainer class with mixed precision [53s]  [211 per 10 epoche]
- Accumulate gradient every 5 batches [53s] [190 per 10 epoche]






Note:

- Custom accuracy performa meglio di torchmetrics accuracy (nel loop vanilla)
- Custom accuracy e torchmetrics accuracy performano (a batch e size significative) meglio in GPU rispetto a CPU
- Num workers con persistent fanno in modo che la prima epoca sia più lenta, ma le successive più veloci (0.5x), quindi è utile nel training di molte
- Batch size più alta rende il training più veloce ma l'influenza dei workers è minore